---
title: "SDS 323 HW 6"
output: html_document
date: "2024-04-24"
---

*Homework 6*

*Samantha Feinberg, Grace Kelley, Samuel Gullion, Ashlynn Sayegh*


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown*
```{r}
library(ggplot2)
library(tidyr)
library(dplyr)
```

Problem 1

5(a) 

```{r}
set.seed(1)
x1 = runif(500) - 0.5 
x2 = runif(500) - 0.5
y=1*(x1^2-x2^2>0)
```


5(b)

```{r}
plot(x1[y==0],x2[y==0],col="red",xlab="X1",ylab="X2")
points(x1[y==1],x2[y==1],col="blue")
```

5(c) 

```{r}
df=data.frame(x1 = x1, x2 = x2, y = as.factor(y))
glm_fit=glm(y~.,data=df, family='binomial')
```

5(d) 

```{r}
glm_prob=predict(glm_fit,newdata=df,type='response')
glm_pred=ifelse(glm_prob>0.5,1,0)
ggplot(data = df, mapping = aes(x1, x2)) +
  geom_point(data = df, mapping = aes(colour = glm_pred))
```


5(e) 

```{r}
glm_fit_2=glm(y~poly(x1,2)+poly(x2,2),data=df,family='binomial')
```

5(f)

```{r}
glm_prob_2=predict(glm_fit_2,newdata=df,type='response')
glm_pred_2=ifelse(glm_prob_2>0.5,1,0)
ggplot(data = df, mapping = aes(x1, x2)) +
  geom_point(data = df, mapping = aes(colour = glm_pred_2))
```


5(g) 

```{r}
library(e1071)
svm_lin=svm(y~.,data=df,kernel='linear',cost=0.01)
plot(svm_lin,df)
```

5(h) 

```{r}
svm_lin_2=svm(y~.,data=df,kernel='radial',gamma=1)
plot(svm_lin_2,data=df)
```

5(i) 

*The method used in part h created the best non-linear boundaries, compared to the other methods throughout problem 5. Part b and d appear to have done the worst job with not much specification.*

Problem 2

6(a) 

```{r}
set.seed(1)
x.one <- runif(500, 0, 90)
y.one <- runif(500, x.one + 10, 100)
x.one.noise <- runif(50, 20, 80)
y.one.noise <- 5/4 * (x.one.noise - 10) + 0.1

x.zero <- runif(500, 10, 100)
y.zero <- runif(500, 0, x.zero - 10)
x.zero.noise <- runif(50, 20, 80)
y.zero.noise <- 5/4 * (x.zero.noise - 10) - 0.1

class.one <- seq(1, 550)
x <- c(x.one, x.one.noise, x.zero, x.zero.noise)
y <- c(y.one, y.one.noise, y.zero, y.zero.noise)

plot(x[class.one], y[class.one], col = "blue", pch = "+", ylim = c(0, 100))
points(x[-class.one], y[-class.one], col = "red", pch = 4)
```

6(b) 

```{r}
set.seed(2)
z <- rep(0, 1100)
z[class.one] <- 1
data <- data.frame(x = x, y = y, z = as.factor(z))
tune.out <- tune(svm, z ~ ., data = data, kernel = "linear", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)))
summary(tune.out)
data.frame(cost = tune.out$performance$cost, misclass = tune.out$performance$error * 1100)
```

6(c) 

```{r}
x.test <- runif(1000, 0, 100)
class.one <- sample(1000, 500)
y.test <- rep(NA, 1000)
# Set y > x for class.one
for (i in class.one) {
    y.test[i] <- runif(1, x.test[i], 100)
}
# set y < x for class.zero
for (i in setdiff(1:1000, class.one)) {
    y.test[i] <- runif(1, 0, x.test[i])
}
plot(x.test[class.one], y.test[class.one], col = "blue", pch = "+")
points(x.test[-class.one], y.test[-class.one], col = "red", pch = 4)
```
```{r}
set.seed(3)
z.test <- rep(0, 1000)
z.test[class.one] <- 1
data.test <- data.frame(x = x.test, y = y.test, z = as.factor(z.test))
costs <- c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)
test.err <- rep(NA, length(costs))
for (i in 1:length(costs)) {
    svm.fit <- svm(z ~ ., data = data, kernel = "linear", cost = costs[i])
    pred <- predict(svm.fit, data.test)
    test.err[i] <- sum(pred != data.test$z)
}
data.frame(cost = costs, misclass = test.err)
```


6(d) 

*The linear kernel seems to overfit the data. When using a small cost, less noisy points are used and the results appear better. But, a bigger cost over-fits the data because it attempts to classify points that are just adding noise to the data.*

Problem 3

7(a) 

```{r}
library(ISLR)
attach(Auto)
Auto$mpglevel <- as.factor(ifelse(Auto$mpg > median(Auto$mpg), 1, 0))
```

7(b)

```{r}
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "linear", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
```

*Using a cost of 1 appears to have the best results.*

7(c)

```{r}
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2, 3, 4)))
summary(tune.out)
```

```{r}
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
```

*When looking at the polynomial kernel, a degree of 2 and cost of 100 performs the best. When looking at the radial kernel a gamma of 0.01 and a cost of 100 performs the best.*

7(d)

```{r}
svm.linear <- svm(mpglevel ~ ., data = Auto, kernel = "linear", cost = 1)
svm.poly <- svm(mpglevel ~ ., data = Auto, kernel = "polynomial", cost = 100, degree = 2)
svm.radial <- svm(mpglevel ~ ., data = Auto, kernel = "radial", cost = 100, gamma = 0.01)
plotpairs = function(fit) {
    for (name in names(Auto)[!(names(Auto) %in% c("mpg", "mpglevel", "name"))]) {
        plot(fit, Auto, as.formula(paste("mpg~", name, sep = "")))
    }
}
plotpairs(svm.linear)
```

Problem 4

8(a) 

```{r}
set.seed(1)
train <- sample(nrow(OJ), 800)
OJ.train <- OJ[train, ]
OJ.test <- OJ[-train, ]
```

8(b) 

```{r}
svm.linear <- svm(Purchase ~ ., data = OJ.train, kernel = "linear", cost = 0.01)
summary(svm.linear)
```
*The number of support vectors is 435 and the number of classes is 2. 219 of the vectors are CH and 216 are MM level.*

8(c) 

```{r}
train.pred <- predict(svm.linear, OJ.train)
table(OJ.train$Purchase, train.pred)
```
```{r}
test.pred <- predict(svm.linear, OJ.test)
table(OJ.test$Purchase, test.pred)
```
```{r}
(75 + 65) / (420 + 240 + 75 + 65)
(33 + 15) / (153 + 69 + 33 + 15)
```
*The training error is 17.5% and the test error is 17.8%.*

8(d) 

```{r}
set.seed(2)
tune.out <- tune(svm, Purchase ~ ., data = OJ.train, kernel = "linear", ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tune.out)
```
*An optimal cost for this data is 0.1.*

8(e)

```{r}
svm.linear <- svm(Purchase ~ ., kernel = "linear", data = OJ.train, cost = tune.out$best.parameter$cost)
train.pred <- predict(svm.linear, OJ.train)
table(OJ.train$Purchase, train.pred)
```
```{r}
test.pred <- predict(svm.linear, OJ.test)
table(OJ.test$Purchase, test.pred)
```
```{r}
(69 + 62) / (423 + 246 + 69 + 62)
(29 + 12) / (156 + 73 + 29 + 12)
```
*The new training error is 16.4% and the new testing error is 15.2%.*

8(f)

```{r}
svm.poly <- svm(Purchase ~ ., kernel = "radial", data = OJ.train, degree = 2)
summary(svm.poly)
```
```{r}
train.pred <- predict(svm.poly, OJ.train)
table(OJ.train$Purchase, train.pred)
```
```{r}
test.pred <- predict(svm.poly, OJ.test)
table(OJ.test$Purchase, test.pred)
```
```{r}
(77 + 44) / (441 + 238 + 77 + 44)
(33 + 17) / (151 + 69 + 33 + 17)
```
*The radial kernel creates 373 support vectors. 188 are CH and 185 are MM. The new training error is 15.1% and the testing error is 18.5%.*

8(g) 

```{r}
set.seed(2)
tune.out <- tune(svm, Purchase ~ ., data = OJ.train, kernel = "polynomial", degree = 2, ranges = list(cost = 10^seq(-2, 
    1, by = 0.25)))
summary(tune.out)
```
```{r}
svm.poly <- svm(Purchase ~ ., kernel = "polynomial", degree = 2, data = OJ.train, cost = tune.out$best.parameter$cost)
summary(svm.poly)
```
```{r}
train.pred <- predict(svm.poly, OJ.train)
table(OJ.train$Purchase, train.pred)
```
```{r}
test.pred <- predict(svm.poly, OJ.test)
table(OJ.test$Purchase, test.pred)
```
```{r}
(90 + 34) / (451 + 225 + 90 + 34)
(41 + 14) / (154 + 61 + 41 + 14)
```
*The polynomial kernel creates 385 support vectors. 197 are CH and 188 are MM. The new training error is 15.5% and the testing error is 20.4%.*

8(h)

*Overall the radial kernel seems to produce the best results with the lowest training and testing errors.*

Problem 5

9(a) 

```{r}
set.seed(2)
hc.complete = hclust(dist(USArrests),method="complete")
plot(hc.complete)
```

9(b) 

```{r}
cutree(hc.complete,3)
```
*The states belonging to each clusters are labeled above. States like New York, New Mexico, or South Carolina belong to 1. States like Rhode Island, Washington, or Wyoming belong to 2. States like North Dakota, Wisconsin, or Maine belong to 3.*

9(c)

```{r}
sd.data = scale(USArrests)
hc.complete.sd = hclust(dist(sd.data),method="complete")
plot(hc.complete.sd)
```

9(d)

```{r}
cutree(hc.complete.sd,3)
```
```{r}
table(cutree(hc.complete,3),cutree(hc.complete.sd,3))
```
*Scaling affects the clusters from the tree and I think the variables should be scaled because the measurements have different units and using the scaling make the clusters easier to understand and analyze.*

Problem 6

10(a)

```{r}
set.seed(2)
x = matrix(rnorm(20*3*50,mean=0,sd=0.001),ncol=50)
x[1:20,2] = 1
x[21:40,1] = 2
x[21:40,2] = 2
x[41:60,1] = 1
true.labels = c(rep(3,20),rep(2,20),rep(1,20))
```

10(b) 

```{r}
pr.out = prcomp(x)
plot(pr.out$x[,1:2],col=1:3,xlab="Z1",ylab="Z2",pch=19)
```

10(c) 

```{r}
km.out = kmeans(x,3,nstart=20)
table(true.labels,km.out$cluster)
```
*The observations appear to be clustered completely correctly.*

10(d) 

```{r}
km.out = kmeans(x,2,nstart=20)
table(true.labels,km.out$cluster)
```
*The clusters that were in 1 of the 3 originally is now in 1 of the 2 created here.*

10(e) 

```{r}
km.out = kmeans(x,4,nstart=20)
table(true.labels,km.out$cluster)
```
*With 4 clusters cluster 2 from before is now split into two separate clusters showing 1 and 2.*

10(f) 

```{r}
km.out = kmeans(pr.out$x[,1:2],3,nstart=20)
table(true.labels,km.out$cluster)
```
*The clusters appear to be correctly clustered again.*

10(g)

```{r}
km.out = kmeans(scale(x),3,nstart=20)
table(true.labels,km.out$cluster)
```
*Scaling affected the distance between the points in the data and resulted in more uneven clusters that do not match those in the true class labels at all.*

Problem 7

13(a)
```{r}
getwd()
```

```{r}
genes = read.csv("C:/Users/saman/Downloads/Ch12Ex13.csv",header=F)
dim(genes)
```

13(b)

```{r}
hc.complete = hclust(as.dist(1-cor(genes)),method="complete")
plot(hc.complete)
```
```{r}
hc.single = hclust(as.dist(1-cor(genes)),method="single")
plot(hc.single)
```
```{r}
hc.average = hclust(as.dist(1-cor(genes)),method="average")
plot(hc.average)
```
*The results definitely depend on the linkage used. Single linkage appears to have two separate clusters, complete displays two more organized clusters, and average gives three.*

13(c) 

```{r}
pr.out = prcomp(t(genes))
head(pr.out$rotation)
```
```{r}
total.load = apply(pr.out$rotation,1,sum)
index = order(abs(total.load),decreasing=TRUE)
index[1:10]
```
*This method shows the 10 most different genes from the groups.*


